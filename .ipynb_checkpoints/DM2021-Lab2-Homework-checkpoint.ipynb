{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Information\n",
    "Name: 黃柏維\n",
    "\n",
    "Student ID: 110062659\n",
    "\n",
    "GitHub ID: v54dt\n",
    "\n",
    "Kaggle name: jimjim951357\n",
    "\n",
    "Kaggle private scoreboard snapshot: \n",
    "\n",
    "[Snapshot1](img/pic0.png)\n",
    "[Snapshot2](img/pic1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First: __This part is worth 30% of your grade.__ Do the **take home** exercises in the [DM2021-Lab2-master Repo](https://github.com/fhcalderon87/DM2021-Lab2-master). You may need to copy some cells from the Lab notebook to this notebook. \n",
    "\n",
    "\n",
    "2. Second: __This part is worth 30% of your grade.__ Participate in the in-class [Kaggle Competition](https://www.kaggle.com/c/dm2021-lab2-hw2/) regarding Emotion Recognition on Twitter. The scoring will be given according to your place in the Private Leaderboard ranking: \n",
    "    - **Bottom 40%**: Get 20% of the 30% available for this section.\n",
    "\n",
    "    - **Top 41% - 100%**: Get (60-x)/6 + 20 points, where x is your ranking in the leaderboard (ie. If you rank 3rd your score will be (60-3)/6 + 20 = 29.5% out of 30%)   \n",
    "    Submit your last submission __BEFORE the deadline (Dec. 24th 11:59 pm, Friday)__. Make sure to take a screenshot of your position at the end of the competition and store it as '''pic0.png''' under the **img** folder of this repository and rerun the cell **Student Information**.\n",
    "    \n",
    "\n",
    "3. Third: __This part is worth 30% of your grade.__ A report of your work developping the model for the competition (You can use code and comment it). This report should include what your preprocessing steps, the feature engineering steps and an explanation of your model. You can also mention different things you tried and insights you gained. \n",
    "\n",
    "\n",
    "4. Fourth: __This part is worth 10% of your grade.__ It's hard for us to follow if your code is messy :'(, so please **tidy up your notebook** and **add minimal comments where needed**.\n",
    "\n",
    "\n",
    "Upload your files to your repository then submit the link to it on the corresponding e-learn assignment.\n",
    "\n",
    "Make sure to commit and save your changes to your repository __BEFORE the deadline (Dec. 29th 11:59 pm, Wednesday)__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin Assignment Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notice\n",
    "I have some difficulty to setup my environment, python 3.7,  tensorflow and keras on M1 mac. So I upload the dataset and write the homework on Kaggle.\n",
    "\n",
    "[Kaggle notebook link](https://www.kaggle.com/jimjim951357/dm-lab2-0110)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exercise 1\n",
    "import nltk\n",
    "\n",
    "train_token =[]\n",
    "\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "\n",
    "for i in train_df['text']:\n",
    "    train_token = train_token+tokenizer.tokenize(i)\n",
    "\n",
    "\n",
    "train_term_freq = nltk.FreqDist(train_token)\n",
    "train_term30 = nltk.FreqDist(dict(train_term_freq.most_common()[:30]))\n",
    "train_term30.plot()\n",
    "\n",
    "\n",
    "test_token =[]\n",
    "\n",
    "\n",
    "\n",
    "for i in test_df['text']:\n",
    "    test_token = test_token+tokenizer.tokenize(i)\n",
    "\n",
    "\n",
    "test_term_freq = nltk.FreqDist(test_token)\n",
    "test_term30 = nltk.FreqDist(dict(test_term_freq.most_common()[:30]))\n",
    "test_term30.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exercise 2\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf = TfidfVectorizer(max_features=1000)\n",
    "txt_fitted = tf.fit(train_df['text'])\n",
    "txt_transformed = txt_fitted.transform(train_df['text'])\n",
    "#print (\"The text: \", train_df['text'])\n",
    "tf.vocabulary_\n",
    "\n",
    "idf = tf.idf_\n",
    "#print(dict(zip(txt_fitted.get_feature_names(), idf)))\n",
    "print(txt_fitted.get_feature_names()[100:110])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exercise 3\n",
    "'''\n",
    "The four diagonal value [58,74,52,44] means that the model prediction was right. Guess the correct emotion.\n",
    "Take anger for exmaple, 58 of 84(58+16+6+4=84) of anger prediction was actually right. But other 26 anger predictions was wrong.\n",
    "Guesses angry but actually fear      11 times.\n",
    "                           joy       7\n",
    "                           sadness   9\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exercise 4\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "naive_model = clf.fit(X_train,y_train)\n",
    "\n",
    "X_train_pred = naive_model.predict(X_train)\n",
    "y_test_pred = naive_model.predict(X_test)\n",
    "\n",
    "acc_train = accuracy_score(y_true = y_train,y_pred = X_train_pred)\n",
    "acc_test = accuracy_score(y_true = y_test,y_pred = y_test_pred)\n",
    "print(\"Train set accuracy:{}\".format(round(acc_train,4)))\n",
    "print(\"Test set accuracy:{}\".format(round(acc_test,4)))\n",
    "\n",
    "\n",
    "print(classification_report(y_true = y_train,y_pred = X_train_pred))\n",
    "print(classification_report(y_true =y_test,y_pred = y_test_pred))\n",
    "\n",
    "my_tags = ['anger', 'fear', 'joy', 'sadness']\n",
    "\n",
    "\n",
    "cm_train = confusion_matrix(y_true = y_train,y_pred = X_train_pred)\n",
    "cm_test = confusion_matrix(y_true =y_test,y_pred = y_test_pred)\n",
    "\n",
    "\n",
    "\n",
    "plot_confusion_matrix(cm_train, classes=my_tags, title='Train Confusion matrix')\n",
    "plot_confusion_matrix(cm_test, classes=my_tags, title='Test Confusion matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exercise 5\n",
    "'''\n",
    "Decision Tree are very flexible and easy to de debug. But it may outfitting easily.\n",
    "Naive Bayes get better score, but it still need more traing dataset. \n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exercise 6\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(training_log['epoch'],training_log['accuracy'],color='b',label='Train accuracy')\n",
    "plt.plot(training_log['epoch'],training_log['val_accuracy'],color='r',label='Val accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accurcy')\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(training_log['epoch'],training_log['loss'],color='b',label='Train loss')\n",
    "plt.plot(training_log['epoch'],training_log['val_loss'],color='r',label='Val loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accurcy')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exercise 7\n",
    "word2vec_model.wv[training_corpus[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercies 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exercise 8\n",
    "# Answer here\n",
    "\n",
    "word_list = ['happy', 'angry', 'data', 'mining']\n",
    "\n",
    "topn = 15\n",
    "happy_words = ['happy'] + [word_ for word_, sim_ in w2v_google_model.most_similar('happy', topn=topn)]\n",
    "angry_words = ['angry'] + [word_ for word_, sim_ in w2v_google_model.most_similar('angry', topn=topn)]        \n",
    "data_words = ['data'] + [word_ for word_, sim_ in w2v_google_model.most_similar('data', topn=topn)]        \n",
    "mining_words = ['mining'] + [word_ for word_, sim_ in w2v_google_model.most_similar('mining', topn=topn)]        \n",
    "\n",
    "print('happy_words: ', happy_words)\n",
    "print('angry_words: ', angry_words)\n",
    "print('data_words: ', data_words)\n",
    "print('mining_words: ', mining_words)\n",
    "\n",
    "target_words = happy_words + angry_words + data_words + mining_words\n",
    "print('\\ntarget words: ')\n",
    "print(target_words)\n",
    "\n",
    "print('\\ncolor list:')\n",
    "cn = topn + 1\n",
    "color = ['b'] * cn + ['g'] * cn + ['r'] * cn + ['y'] * cn\n",
    "print(color)\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "## w2v model\n",
    "model = w2v_google_model\n",
    "\n",
    "## prepare training word vectors\n",
    "size = 200\n",
    "target_size = len(target_words)\n",
    "all_word = list(model.key_to_index)\n",
    "word_train = target_words + all_word[:size]\n",
    "X_train = model[word_train]\n",
    "\n",
    "## t-SNE model\n",
    "tsne = TSNE(n_components=2, metric='cosine', random_state=28)\n",
    "\n",
    "## training\n",
    "X_tsne = tsne.fit_transform(X_train)\n",
    "\n",
    "## plot the result\n",
    "plt.figure(figsize=(7.5, 7.5), dpi=115)\n",
    "plt.scatter(X_tsne[:target_size, 0], X_tsne[:target_size, 1], c=color)\n",
    "for label, x, y in zip(target_words, X_tsne[:target_size, 0], X_tsne[:target_size, 1]):\n",
    "    plt.annotate(label, xy=(x,y), xytext=(0,0),  textcoords='offset points')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle name: jimjim951357\n",
    "\n",
    "Kaggle private scoreboard snapshot: \n",
    "\n",
    "[Snapshot1](img/pic0.png)\n",
    "[Snapshot2](img/pic1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing:\n",
    "data structure:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```python\n",
    "train_df = pandas.DataFrame()\n",
    "test_df = pandas.DataFrame()\n",
    "\n",
    "train_tweet_id = []\n",
    "train_text = []\n",
    "train_emotion = []\n",
    "\n",
    "test_tweet_id = []\n",
    "test_text = []\n",
    "```\n",
    "\n",
    "constuct two pandas dataframe to store train data and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some tweet in the dataset are not in the identification. So we iterate through data_identification.csv rather than tweets_DM.json. Then split it to train and test dataframe by attribute in data_identification.csv.\n",
    "\n",
    "```python\n",
    "for k in data_identification.keys():\n",
    "    if data_identification[k] == \"train\":\n",
    "        if k in tweets_user:\n",
    "            train_tweet_id.append(k)\n",
    "            train_text.append(tweets_user[k])\n",
    "            train_emotion.append(emotion[k])\n",
    "            \n",
    "    elif data_identification[k] == 'test':\n",
    "        if k in tweets_user:\n",
    "            test_tweet_id.append(k)\n",
    "            test_text.append(tweets_user[k])\n",
    "            #test_emotion.append(emotion[k])\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Optimization:\n",
    "train_df are use to train model. test_df are only use for prediction. \n",
    "I modify the train_df \\` split test size and the decision tree parameters to get a better precision score on Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I only use decision tree model in the competition. I got very low precision 0.12 at first which was very frustrated. I tried to modeify the parameter then finally gets a bet score, though 0.28 is still lower than others. After the public dataset subimission ended, I tried to use neural network. But I failed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forth Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[kaggle competition notebook](https://www.kaggle.com/jimjim951357/0107copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tweet text preprocessing function\n",
    "from string import punctuation\n",
    "\n",
    "import re\n",
    "\n",
    "def remove_emojis(data):\n",
    "    emoj = re.compile(\"[\"\n",
    "        u\"\\U00002700-\\U000027BF\"  # Dingbats\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # Emoticons\n",
    "        u\"\\U00002600-\\U000026FF\"  # Miscellaneous Symbols\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # Miscellaneous Symbols And Pictographs\n",
    "        u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "        u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # Transport and Map Symbols\n",
    "                      \"]+\", re.UNICODE)\n",
    "    return re.sub(emoj, '', data)\n",
    "\n",
    "\n",
    "def remove_punctuation_emoji(value):\n",
    "    value = remove_emojis(value)\n",
    "    return ''.join([c for c in value if (c not in punctuation)])\n",
    "\n",
    "## read file\n",
    "import json\n",
    "with open(\"../input/dm2021-lab2-hw2/tweets_DM.json\",\"r\") as file:\n",
    "    tweets_json_list = [eval(f) for f in file.readlines()]\n",
    "\n",
    "tweets_user = {u['_source'][\"tweet\"][\"tweet_id\"]:remove_punctuation_emoji(u['_source'][\"tweet\"][\"text\"]) for u in tweets_json_list}\n",
    "\n",
    "import csv\n",
    "data_identification = None\n",
    "\n",
    "with open(\"../input/dm2021-lab2-hw2/data_identification.csv\",newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    _ = next(reader) # iterate once to skip the first line\n",
    "    data_identification = {rows[0]: rows[1] for rows in reader} \n",
    "\n",
    "emotion = None\n",
    "with open(\"../input/dm2021-lab2-hw2/emotion.csv\",newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    _ = next(reader)\n",
    "    emotion = {rows[0]:rows[1] for rows in reader} #row[0]:key row[1]:data, dict as a key value\n",
    "    \n",
    "    \n",
    "\n",
    "## train and test data construction\n",
    "import pandas\n",
    " \n",
    "train_df = pandas.DataFrame()\n",
    "test_df = pandas.DataFrame()\n",
    "\n",
    "train_tweet_id = []\n",
    "train_text = []\n",
    "train_emotion = []\n",
    "\n",
    "test_tweet_id = []\n",
    "test_text = []\n",
    " \n",
    "## pick up the train text and test text\n",
    "for k in data_identification.keys():\n",
    "    #print(k)\n",
    "    if data_identification[k] == \"train\":\n",
    "        if k in tweets_user:\n",
    "            train_tweet_id.append(k)\n",
    "            train_text.append(tweets_user[k])\n",
    "            train_emotion.append(emotion[k])\n",
    "            \n",
    "    elif data_identification[k] == 'test':\n",
    "        if k in tweets_user:\n",
    "            test_tweet_id.append(k)\n",
    "            test_text.append(tweets_user[k])\n",
    "            #test_emotion.append(emotion[k])\n",
    "\n",
    "\n",
    "train_df = pandas.DataFrame(list(zip(train_tweet_id, train_text,train_emotion)),\n",
    "               columns =['tweet_id', 'text','emotion'])\n",
    "\n",
    "## BOW transform\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "BOW_vectorizer = CountVectorizer() \n",
    "\n",
    "#train_data_BOW_features.toarray()\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# build analyzers (bag-of-words)\n",
    "BOW_500 = CountVectorizer(max_features=500, tokenizer=nltk.word_tokenize) \n",
    "\n",
    "# apply analyzer to training data\n",
    "BOW_500.fit(train_df['text'])\n",
    "\n",
    "train_data_BOW_features_500 = BOW_500.transform(train_df['text'])\n",
    "\n",
    "## check dimension\n",
    "#train_data_BOW_features_500.shape\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_df['text'], train_df['emotion'], test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# for a classificaiton problem, you need to provide both training & testing data\n",
    "X_train = BOW_500.transform(X_train)\n",
    "#y_train = train_df['emotion']\n",
    "\n",
    "X_test = BOW_500.transform(X_test)\n",
    "#y_test = test_df['emotion']\n",
    "\n",
    "## take a look at data dimension is a good habbit  :)\n",
    "print('X_train.shape: ', X_train.shape)\n",
    "print('y_train.shape: ', y_train.shape)\n",
    "print('X_test.shape: ', X_test.shape)\n",
    "print('y_test.shape: ', y_test.shape)\n",
    "\n",
    "## build DecisionTree model\n",
    "DT_model = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "## training!\n",
    "DT_model = DT_model.fit(X_train, y_train)\n",
    "\n",
    "## predict!\n",
    "y_train_pred = DT_model.predict(X_train)\n",
    "y_test_pred = DT_model.predict(X_test)\n",
    "\n",
    "## so we get the pred result\n",
    "y_test_pred[:10]\n",
    "\n",
    "## accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "acc_train = accuracy_score(y_true=y_train, y_pred=y_train_pred)\n",
    "acc_test = accuracy_score(y_true=y_test, y_pred=y_test_pred)\n",
    "\n",
    "print('training accuracy: {}'.format(round(acc_train, 2)))\n",
    "print('testing accuracy: {}'.format(round(acc_test, 2)))\n",
    "\n",
    "## precision, recall, f1-score,\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_true=y_test, y_pred=y_test_pred))\n",
    "\n",
    "#X_test = BOW_500.transform(X_test)\n",
    "#y_test_pred = DT_model.predict(X_test)\n",
    "\n",
    "test_text = BOW_500.transform(test_text)\n",
    "test_emotion_pred = DT_model.predict(test_text)\n",
    "\n",
    "## save to file\n",
    "\n",
    "save_data_decisiontree = pandas.DataFrame({\"id\":test_tweet_id,\"emotion\":test_emotion_pred})\n",
    "save_data_decisiontree.to_csv(\"./Submission_decisiontree5.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
